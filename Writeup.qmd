---
title: "Writeup"
format: html
execute:
  echo: false
  output: false
---

Andy Fan (Section 2: M/W 10:30-11:50), Git: programming-af

Juan Ulloa (Section 2: M/W 10:30-11:50), Git: jmulloa25

Will Sigal (Section 2: M/W 10:30-11:50), Git: willsigal 

### Data Links (includes datasets too large for "Data" folder)

ORIGINAL UNPROCESSED DATA:

https://drive.google.com/drive/folders/1Mfv-cvmm9iIgnQ7UKKiMuoTJo0Vjpcv4?usp=sharing

CLEANED DATA:

https://drive.google.com/drive/folders/1los_5JwF1rNVOaFtjH1B2Z8thgmBXHxC?usp=drive_link

### Research Question:
We were interested in how gang violence/armed conflict can impact economic development. Specifically, our research question was how decrease in gang violence can effect the economic development indicators of El-Salvador and by extention Central America.

We were able to obtain ACLED data that allowed us to identify periods of discontinuity in levels of violence in El-salvador. The "Territorial Control Plan" implemented in 2019 by Nayib Bukele led to a drastic decrease in total violence, gang vaiolence, and violence involving civilians. Which allowed us to make comparisons of economic/development statistics before and after this period by also using World Bank development statstics.

### Coding and Approach:
We utilized 2 ACLED violent events datasets and 2 World Bank Development indicators datasets to do our analysis. Our broad approach was to: 1.perform exploritory analysis on all datasets, identify useful data, trends, and graphs. 2.analyse trends in violence and make sure there is actually substaintial support for a reduction in violence related to policy events. 3.Seek to dissect and better understand the categories, actors, etc. of violence. 4.join ACLED and world bank data, to see how violence impacts development indicators.

### coding descriptions 

*(All the different qmd's codes is copied below in this document, after the 'Directions for Future Work' section)

1.final_project_data_clean.qmd -- cleans all the datasets (merges, na removes, pivots, remove redundant columns etc.) from raw data into usable data that the other qmds load

2.Economic_Analysis.qmd -- data analysis using world bank development indicators such as GDP, homicides, graph correlation plots and indicators over time. Also graphing development indicators vs violence by merging World Bank and ACLED datasets, such as graphs of Birth/Death Rates vs #violent events from 2018-2023. (also containes some code related to shapefile cleaning used later in shiny app)

3.EDA_Analysis.qmd -- data anslysis primarily on acled violecnce data and el-salvador specific data. Includes bar and pie charts that breakdown violent events by types, actors etc. Includes plots related to violence for both Central America and El-Salvador specifically. Includes natural language modeling using package 'gensim' to see how descriptions of events and their prevalence change over time. Also includes mapping for shiny app.

4.Time_Series_Analysis -- graphs of violence in central america and el-salvador, including lines of best fit, shaded areas for impactful events etc.

5.app.py -- Our Shiny app: Contains dynamic plot/shapefile that shows changes in indicators over time. Country/Region and indicators all selectable with dropdown menu.

### static plots
(We produced over a dozen static plots across all code, selected 2 to show as an example):

![Graph of Violence across time in El-Salvador, includes key events and lines of best fit](pictures/Monthly_Attacks_ELsalv_regression.jpg)

![Graph of relation between Central American GDP and total number of violent events](pictures/Violence_GDP.jpg)

### Directions for future work
We will definetly want to further explore how violence effects more types of economics indicators. There are hundereds of economic indicators provided by the world bank, we simply did not have time to examine most of them. Given more time we would like to explore this further.

We also want to see how different types of violence correlate with these indicators (eg does police violence affect certian indicators a different way than gang violence?)

Our analysis also identified an increase in violence in cournties surrounding El-Salvador whilst El-Salvador's own violence rate decreased. This may be a spillover effect that we would certianly like to investigate more. 

We would also want to add regression analysis to our study, such that we can skee to infer potential causual relations instead of just correlation.

#### (code below -- code chunks hidden)
---YOU MUST IMPUT YOUR OWN DIRECTORIES AT LINE 83, 991, AND 1200---

final_project_data_clean:
```{python}
### SETUP 
import pandas as pd
import altair as alt
import time
import os
import warnings
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
import requests
from bs4 import BeautifulSoup
import concurrent.futures

### SETTING WD -- CHANGE DIRECTORY HERE
os.chdir('d:\\UChicago\\Classes\\2024Qfall\\Programming Python\\Final-Project\\Data') # change directory here
#os.chdir("C:\Users\jmull\Downloads\ACLED_2017-2024-Q1-QTA.csv")  #juan wd
#os.chdir("/Users/willsigal/Desktop/UChicago/Fall 2025/Python Final")   #will wd

### --- ###

# there is another directory you must change at line 991
# there is another directory you must change at line 1200
```

```{python}
wbcm = pd.read_csv('central_america_data_combined.csv', header=None, skiprows=4)

# remove empty space, set header, remove years before 2000, and empty variables
wbcm.columns = wbcm.iloc[0]
wbcm = wbcm[1:].reset_index(drop=True)
wbcm = wbcm.drop(wbcm.columns[4:44], axis=1)

wbcm = wbcm.loc[~wbcm.iloc[:, 4:].isna().all(axis=1)] # only drops columns that are fully empty--basically a less harsh dropna

# melting
wbcm_melt = wbcm.melt(id_vars=['Country Name','Country Code','Indicator Name','Indicator Code'], var_name='Year', value_name='Value')
wbcm_melt = wbcm_melt.drop('Indicator Code', axis=1)

# pivot
wbcm_pv = wbcm_melt.pivot_table(index=['Country Name', 'Year'], columns='Indicator Name', values='Value', aggfunc='first').reset_index()

```

```{python}
# remove na (if column has more than 10% values missing then drop)
wbcm_pv = wbcm_pv.loc[:, wbcm_pv.isnull().mean() <= 0.1]

# filter by keywords
keywords = ['Country','Year', "employment", "debt", "tax", "gdp", 'homicide','death','birth','expenditures','public','education']
wbcm_pv_pattern = "|".join(keywords)
wbcm_pv = wbcm_pv.filter(regex=wbcm_pv_pattern, axis=1)

exclude_keywords = ["modeled", "external debt",'multilateral','Short-term']
wbcm_pv_excluded = "|".join(exclude_keywords)  # "modeled|external debt"

wbcm_pv = wbcm_pv.loc[:, ~wbcm_pv.columns.str.contains(wbcm_pv_excluded, case=False)]
```

```{python}
# export
wbcm_pv.to_csv("central_america_data_cleaned.csv", index=False)
```

```{python}
wbdi = pd.read_csv('world_bank_development_indicators.csv')

# subset to central american countries, and post 2000
countries_list = ['Belize', 'Costa Rica', 'El Salvador', 'Guatemala', 'Honduras', 'Mexico', 'Nicaragua', 'Panama']
wbdi_CA = wbdi[wbdi['country'].isin(countries_list)]

wbdi_CA['date'] = pd.to_datetime(wbdi_CA['date'])
wbdi_CA = wbdi_CA[wbdi_CA['date'] > '2000-01-01']
```

```{python}
# subset by relevant columns
wbdi_CA_columns = ['country','date','land_area','intentional_homicides','GDP_current_US','population','life_expectancy_at_birth','access_to_electricity%','inflation_annual%','gini_index','human_capital_index','intentional_homicides','CO2_emisions','real_interest_rate','tax_revenue%','expense%','political_stability_estimate','rule_of_law_estimate','life_expectancy_at_birth','population']

wbdi_CA = wbdi_CA[wbdi_CA_columns]
```

```{python}
# export
wbdi_CA.to_csv("world_bank_development_indicators_cleaned.csv", index=False)
```

```{python}
### ACLED import (simplified so only 1 line is needed to be changed)

acled = pd.read_csv('ACLED_2017-2024-Q1-QTA.csv')

# Subset by Central American countries
acled_CA = acled[acled["region"] == "Central America"]
```

```{python}
# Subset by relevant variables
acled_columns = ['year',"event_id_cnty", "event_date", "event_type", 'location',
           "sub_event_type", "actor1", "actor2", 
           "inter1", "inter2", "interaction", 
           "country", "admin1", "latitude", 
           "longitude", "geo_precision", "notes"]

acled_CA = acled_CA[acled_columns]
```

```{python}
# export
acled_CA.to_csv('acled_project_data_cleaned.csv', index=False)
```

Economic_Analysis
```{python}
### LOAD DATA
wbcm_pv = pd.read_csv('central_america_data_cleaned.csv') # world bank dataset from kaggle
wbdi_CA = pd.read_csv('world_bank_development_indicators_cleaned.csv') # world bank dataset from world bank
acled_CA = pd.read_csv('acled_project_data_cleaned.csv') # ACLED
```

```{python}
print(wbdi_CA.columns)
print(wbdi_CA.isnull().sum()) # Check for missing values
```

```{python}
print(wbcm_pv.columns)
print(wbcm_pv.isnull().sum())
```

```{python}
print(acled_CA.columns)
print(acled_CA.isnull().sum())
```

```{python}
#Rename Columns for merges and such
wbdi_CA.rename(columns={'country': 'Country', 'date': 'Year'}, inplace=True)
wbcm_pv.rename(columns={'Country Name': 'Country', 'Year': 'Year'}, inplace=True)
acled_CA.rename(columns={'country': 'Country', 'year': 'Year'}, inplace=True)

# cleaning dtypes
acled_CA['Year'] = pd.to_numeric(acled_CA['Year'], errors='coerce', downcast='integer')
print(acled_CA['Year'].head())

# make non datetime dates to datetime
acled_CA['Year'] = pd.to_datetime(acled_CA['event_date'], errors='coerce').dt.year
wbdi_CA['Year'] = pd.to_datetime(wbdi_CA['Year'], errors='coerce')

# cleaning year dtypes
acled_CA['Year'] = acled_CA['Year'].astype(int)
wbdi_CA['Year'] = wbdi_CA['Year'].dt.year
wbdi_CA['Year'] = wbdi_CA['Year'].astype(int)
wbcm_pv['Year'] = wbcm_pv['Year'].astype(int)
```

```{python}
# Basic statistics by country
wbdi_summary = wbdi_CA.groupby('Country').mean()
print(wbdi_summary)

# Identify indicators with the most missing values
nonmissing_wbdi = wbdi_CA.isnull().sum().sort_values(ascending=True)
print("Top indicators in WBDI:\n", nonmissing_wbdi.head(10))
```

```{python}
## Political Stability
alt.Chart(wbdi_CA).mark_line().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('political_stability_estimate', title='Political Stability by Year'),
    color='Country:N',
    tooltip=['Country', 'Year', 'political_stability_estimate']
).properties(
    title='Political Stability Country'
)

```

```{python}
# Filter out Mexico
filtered_data = wbdi_CA[wbdi_CA['Country'] != 'Mexico']

# Plot GDP per Capita Over Time without Mexico
alt.Chart(filtered_data).mark_line().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('GDP_current_US:Q', title='GDP (Current US$)'),
    color='Country:N',
    tooltip=['Country', 'Year', 'GDP_current_US']
)
```

```{python}
#Homicide Rates
alt.Chart(wbdi_CA).mark_line().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('intentional_homicides:Q', title='Homicide Rate (Total)'),
    color='Country:N',
    tooltip=['Country', 'Year', 'intentional_homicides']
)

#Notice Large Drop in Homicide Rates in El Salvador
```

```{python}
# Select a subset of indicators
selected_indicators = wbdi_CA[[
    'GDP_current_US',
    'population',
    'life_expectancy_at_birth',
    'access_to_electricity%',
    'inflation_annual%',
    'gini_index',
    'human_capital_index',
    'intentional_homicides'
]]

# Drop rows with missing values
selected_indicators = selected_indicators.dropna()

# Compute the correlation matrix
corr_matrix = selected_indicators.corr()

# Reset index for Altair
corr_matrix = corr_matrix.reset_index().melt('index')

# Create the correlation heatmap
corr_heatmap = alt.Chart(corr_matrix).mark_rect().encode(
    x=alt.X('index:N', title='Indicator'),
    y=alt.Y('variable:N', title='Indicator'),
    color=alt.Color('value:Q', scale=alt.Scale(scheme='redblue', domain=(-1, 1))),
    tooltip=['index:N', 'variable:N', 'value:Q']
).properties(
    title='Correlation Matrix of Development Indicators',
    width=400,
    height=400
)

corr_heatmap.display()
```

```{python}
import folium
from folium.plugins import MarkerCluster
from shapely.geometry import Point

# Ensure latitude and longitude column names are correct
latitude_column = 'latitude'
longitude_column = 'longitude'

# Create geometry for the GeoDataFrame
geometry = [Point(xy) for xy in zip(acled_CA[longitude_column], acled_CA[latitude_column])]
acled_gdf = gpd.GeoDataFrame(acled_CA, geometry=geometry, crs='EPSG:4326')
```

```{python}
#shapefile_path = '/Users/willsigal/Desktop/UChicago/Fall 2025/Python Final/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp'    # will wd
shapefile_path = "d:\\UChicago\\Classes\\2024Qfall\\Programming Python\\Final-Project\\ne_110m_admin_0_countries\\ne_110m_admin_0_countries.shp"    # andy wd

latin_america = gpd.read_file(shapefile_path)
countries_in_data = acled_CA['Country'].unique()
latin_america = latin_america[latin_america['ADMIN'].isin(countries_in_data)]
latin_america = latin_america.to_crs(epsg=3857)

event_types = acled_gdf['event_type'].unique()
num_event_types = len(event_types)
```

```{python}
# Merge ACLED and WBDI(central america)
el_salvador_violence = acled_CA[acled_CA['Country'] == 'El Salvador']
el_salvador_gdp = wbdi_CA[wbdi_CA['Country'] == 'El Salvador']

# Group violence data by year
violence_per_year = el_salvador_violence.groupby('Year').size().reset_index(name='Violent Events')

# Group GDP data by year
avg_gdp_per_year = el_salvador_gdp.groupby('Year')['GDP_current_US'].mean().reset_index(name='Average GDP')

# Merge on Year
violence_gdp_time = pd.merge(violence_per_year, avg_gdp_per_year, on='Year')

# Verify merged data
print(violence_gdp_time.head())
```

```{python}
# Plotting
fig, ax1 = plt.subplots(figsize=(9, 4))

color = 'tab:red'
ax1.set_xlabel('Year')
ax1.set_ylabel('Violent Events', color=color)
ax1.plot(violence_gdp_time['Year'], violence_gdp_time['Violent Events'], color=color)
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis

color = 'tab:blue'
ax2.set_ylabel('Average GDP (US$)', color=color)
ax2.plot(violence_gdp_time['Year'], violence_gdp_time['Average GDP'], color=color)
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Violent Events and Average GDP Over Time')
fig.tight_layout()
plt.xticks(rotation=45)
plt.show()
```

```{python}
print(wbdi_CA['Year'].head(10))
print(wbdi_CA['Year'].dtype)
```

```{python}
# Altair plot for Violent Events and Average GDP
base = alt.Chart(violence_gdp_time).encode(
    x=alt.X('Year:O', title='Year')  # Treating Year as ordinal for cleaner spacing
)

# Line chart for Violent Events
line_violence = base.mark_line(color='red').encode(
    y=alt.Y('Violent Events:Q', title='Violent Events'),
    tooltip=['Year', 'Violent Events']
)

# Line chart for Average GDP
line_gdp = base.mark_line(color='blue').encode(
    y=alt.Y('Average GDP:Q', title='Average GDP (US$)', axis=alt.Axis(grid=False)),
    tooltip=['Year', 'Average GDP']
)

# Combine the two charts
el_salv_chart = alt.layer(line_violence, line_gdp).resolve_scale(
    y='independent'  # Ensure each Y-axis is independent
).properties(
    title='Violent Events and Average GDP in El Salvador Over Time',
    width=900,
    height=400
)
# suggest adding a tooltip here to make clear which line is which

el_salv_chart
```

```{python}
#### preparing data for merge
# acled data (counts of violence) group by year 
acled_CA_gp = acled_CA.groupby(['Country', 'Year']).size().reset_index(name='violence_events')
acled_CA_gp = acled_CA_gp[acled_CA_gp['Country'] != 'Mexico'] 
acled_CA_gp = acled_CA_gp[acled_CA_gp['Year'] <= 2022]

# world bank data
wbcm_pv_gp = wbcm_pv[(wbcm_pv['Year'] >= 2018) & (wbcm_pv['Year'] <= 2022)]
wbcm_pv_gp = wbcm_pv_gp[wbcm_pv_gp['Country'] != 'Mexico'] 

# merge
crime_effects_development = pd.merge(acled_CA_gp, wbcm_pv_gp, on=['Country', 'Year'], how='inner')
crime_effects_development.columns
```

```{python}
# violence and birth rates
violence_birth_df = crime_effects_development.groupby('Year').agg({
    'violence_events': 'sum',
    'Fertility rate, total (births per woman)': 'mean'
}).reset_index()

# graph
violence_ch = alt.Chart(violence_birth_df).mark_line(color='red').encode(
    x=alt.X('Year:O', title='Year', axis=alt.Axis(labelAngle=0)),
    y=alt.Y('violence_events:Q', axis=alt.Axis(title='Violent events (Total)', labelColor='red'), scale=alt.Scale(domain=[1500, 2800])),
    tooltip=[ 'Year', 'violence_events']
).properties(title='Violence Over Time',width=900, height=400)

birth_ch = alt.Chart(violence_birth_df).mark_line(color='green').encode(
    x=alt.X('Year:O', title='Year', axis=alt.Axis(labelAngle=0)),
    y=alt.Y('Fertility rate, total (births per woman):Q', axis=alt.Axis(title='Births Per Woman', labelColor='green'),scale=alt.Scale(domain=[1.9, 2.4])),
    tooltip=['Year', 'Fertility rate, total (births per woman)']
).properties(title='Birth Rates Over Time',width=900, height=400)

violence_birth = alt.layer(violence_ch, birth_ch).resolve_scale(
    y='independent'  # Ensure each Y-axis is independent
).properties(
    title='Violent Events and Birth Rates Over Time',
    width=900,
    height=400
)
violence_birth
```

```{python}
# violence and child death rates
violence_death_df = crime_effects_development.groupby('Year').agg({
    'violence_events': 'sum',
    'Number of under-five deaths': 'sum'
}).reset_index()

# graph
death_ch = alt.Chart(violence_death_df).mark_line(color='blue').encode(
    x=alt.X('Year:O', title='Year', axis=alt.Axis(labelAngle=0)),
    y=alt.Y('Number of under-five deaths:Q', axis=alt.Axis(title='Under 5y/o Deaths (Total)', labelColor='blue'),scale=alt.Scale(domain=[16000, 21000])),
    tooltip=['Year', 'Number of under-five deaths']
).properties(title='Under 5 Deaths Over Time',width=900, height=400)

violence_death = alt.layer(violence_ch, death_ch).resolve_scale(
    y='independent'  # Ensure each Y-axis is independent
).properties(
    title='Violent Events and Under-5y/o Death Rates Over Time',
    width=900,
    height=400
)
violence_death
```

```{python}
# violence and debt payments
violence_debt_df = crime_effects_development.groupby('Year').agg({
    'violence_events': 'sum',
    'Total debt service (% of GNI)': 'mean'
}).reset_index()

# graph
debt_ch = alt.Chart(violence_debt_df).mark_line(color='orange').encode(
    x=alt.X('Year:O', title='Year', axis=alt.Axis(labelAngle=0)),
    y=alt.Y('Total debt service (% of GNI):Q', axis=alt.Axis(title='Debt Payments (%GNI)', labelColor='orange'), scale=alt.Scale(domain=[6, 14])),
    tooltip=['Year', 'Total debt service (% of GNI)']
).properties(title='Debt Payments Over Time',width=900, height=400)

violence_debt = alt.layer(violence_ch, debt_ch).resolve_scale(
    y='independent'  # Ensure each Y-axis is independent
).properties(
    title='Violent Events and National Debt Payment Over Time',
    width=900,
    height=400
)
violence_debt
```

```{python}
# violence and education
violence_edu_df = crime_effects_development.groupby('Year').agg({
    'violence_events': 'sum',
    'Primary education, pupils': 'sum'
}).reset_index()

# graph
edu_ch = alt.Chart(violence_edu_df).mark_line(color='teal').encode(
    x=alt.X('Year:O', title='Year', axis=alt.Axis(labelAngle=0)),
    y=alt.Y('Primary education, pupils:Q', axis=alt.Axis(title='Total number of Students', labelColor='teal'), scale=alt.Scale(domain=[5800000, 6100000])),
    tooltip=['Year', 'Primary education, pupils']
).properties(title='Education over time',width=900, height=400)

violence_edu = alt.layer(violence_ch, edu_ch).resolve_scale(
    y='independent'  # Ensure each Y-axis is independent
).properties(
    title='Violent Events and Number of Students over time',
    width=900,
    height=400
)
violence_edu
```

EDA_Analysis
```{python}
import pandas as pd
import altair as alt

CA_ACLED = pd.read_csv('acled_project_data_cleaned.csv')
```

```{python}
required_columns = ['year', 'event_type', 'location']
if not all(col in CA_ACLED.columns for col in required_columns):
    raise ValueError(f"The dataset must include the columns: {', '.join(required_columns)}")

```

```{python}
CA_ACLED['year'] = CA_ACLED['year'].astype(int)
CA_ACLED.dropna(subset=['event_type'], inplace=True)
CA_ACLED = CA_ACLED[CA_ACLED['country'] != 'Mexico']
# Aggregate data to get total crimes per year per country
aggregated_data = (
    CA_ACLED.groupby(['country', 'year'])
    .size()
    .reset_index(name='total_crimes')
)

bar_chart = alt.Chart(aggregated_data).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('total_crimes:Q', title='Total Crimes'),
    color=alt.Color('country:N', title='Country'),
    tooltip=['country', 'year', 'total_crimes']
).properties(
    title="Total Crimes Per Year Per Country",
    width=600,
    height=400
)
bar_chart
```

```{python}
line_chart = alt.Chart(aggregated_data).mark_line(point=True).encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('total_crimes:Q', title='Total Crimes'),
    color=alt.Color('country:N', title='Country'),
    tooltip=['country', 'year', 'total_crimes']
).properties(
    title="Total Crimes Per Year Per Country",
    width=600,
    height=400
)
line_chart
```

```{python}
wbdi = pd.read_csv('world_bank_development_indicators_cleaned.csv')

wbdi['year'] = pd.to_datetime(wbdi['date']).dt.year

wbdi = wbdi.rename(columns={'population': 'Population'})

merged_data = pd.merge(aggregated_data, wbdi[['country', 'year', 'Population']],
                       on=['country', 'year'], 
                       how='left')


merged_data['crimes_per_person'] = merged_data['total_crimes'] / merged_data['Population']

line_chart = alt.Chart(merged_data).mark_line(point=True).encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('crimes_per_person:Q', title='Crimes per Person'),
    color=alt.Color('country:N', title='Country'),
    tooltip=['country', 'year', 'crimes_per_person']
).properties(
    title="Crimes Per Person Per Year Per Country",
    width=400,
    height=400
)

# Display the chart
line_chart
```

```{python}
wbdi.columns
```

```{python}
wbdi['homicide_per_person'] = wbdi['intentional_homicides'] / wbdi['Population']

line_chart_wbdi = alt.Chart(wbdi).mark_line(point=True).encode(
    x=alt.X('date:T', title = 'Year'),
    y=alt.Y('homicide_per_person:Q', title= 'Homicide Per Person'),
    color=alt.Color('country:N', title= 'Country'),
    tooltip=['country', 'date', 'homicide_per_person']
).properties(
    title="Crimes Per Person Per Year Per Country",
    width=400,
    height=400
)

line_chart_wbdi

```

```{python}
# Filter data for El Salvador
el_salvador_data = CA_ACLED[CA_ACLED['country'] == 'El Salvador']

# Aggregate data to get total crimes per year by event type
crime_type_data = (
    el_salvador_data.groupby(['year', 'event_type'])
    .size()
    .reset_index(name='crime_count')
)

# Create the stacked bar chart
stacked_bar_chart = alt.Chart(crime_type_data).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('crime_count:Q', title='Number of Crimes'),
    color=alt.Color('event_type:N', title='Crime Type'),
    tooltip=['year', 'event_type', 'crime_count']
).properties(
    title="Crimes Per Year by Type in El Salvador",
    width=600,
    height=400
)

# Display the chart
stacked_bar_chart
```

```{python}
acled_2013 = pd.read_csv('2013-01-01-2024-01-01-Central_America.csv')
```

```{python}
missing_columns = [col for col in CA_ACLED.columns if col not in acled_2013.columns]

print("Columns in df2 but not in df1:", missing_columns)
```

```{python}
print(acled_2013.columns)

print(CA_ACLED.columns)
```

```{python}
el_salvador_data_2 = acled_2013[acled_2013['country'] == 'El Salvador']

crime_event_type_subtype_data = (
    el_salvador_data.groupby(['year', 'event_type', 'sub_event_type'])
    .size()
    .reset_index(name='count_event_count')
)
```

```{python}
# Create the bar chart
bar_chart_crime = alt.Chart(crime_event_type_subtype_data).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('count_event_count:Q', title='Total Crimes'),
    color=alt.Color('event_type', title='Crime Type'),
    tooltip=['count_event_count', 'year', 'event_type']
).properties(
    title="Total Crimes in El Salvador: Per Year Per Type",
    width=600,
    height=400
)

# Display the chart
bar_chart_crime
```

```{python}
# Create the bar chart
bar_chart_crime = alt.Chart(crime_event_type_subtype_data).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('count_event_count:Q', title='Total Crimes'),
    color=alt.Color('sub_event_type', title='Crime Sub-Type'),
    tooltip=['count_event_count', 'year', 'sub_event_type']
).properties(
    title="Total Crimes in El Salvador: Per Year Per Sub-Type",
    width=600,
    height=400
)

# Display the chart
bar_chart_crime
```

```{python}
pie_charts = []

# Loop through each unique year and create a pie chart
for year in crime_event_type_subtype_data['year'].unique():
    year_data = crime_event_type_subtype_data[crime_event_type_subtype_data['year'] == year]
    
    pie_chart = alt.Chart(year_data).mark_arc().encode(
        theta=alt.Theta(field="count_event_count", type="quantitative"),
        color=alt.Color(field="sub_event_type", type="nominal", legend=alt.Legend(title="Sub Event Type")),
        tooltip=[
            alt.Tooltip("event_type:N", title="Event Type"),
            alt.Tooltip("sub_event_type:N", title="Sub Event Type"),
            alt.Tooltip("count_event_count:Q", title="Event Count")
        ]
    ).properties(
        title=f"Crime Event Distribution in El Salvador ({year})"
    )
    
    pie_charts.append(pie_chart)

# Display all pie charts
alt.vconcat(*pie_charts)
```

```{python}
actors_df = (el_salvador_data_2.groupby(['year', 'actor1'])
    .size()
    .reset_index(name='count_actors')
)

unique_actors = actors_df['actor1'].nunique()
```

```{python}
print(f"Total unique actors: {unique_actors}")

## Too many actors

actor_counts = (
    el_salvador_data_2.groupby('actor1')
    .size()
    .reset_index(name='total_count')
    .sort_values(by='total_count', ascending=False)
)

## Lets filter out actors that appear less than 10 times through the years:

threshold = 10 
actor_counts['cleaned_actor1'] = actor_counts['actor1'].where(
    actor_counts['total_count'] >= threshold, 'Other'
)

cleaned_actor_counts = (
    actor_counts.groupby('cleaned_actor1')['total_count']
    .sum()
    .reset_index()
    .sort_values(by='total_count', ascending=False)
)

print(cleaned_actor_counts)
```

```{python}
actor_mapping = {
    'Police Forces of El Salvador (2019-)': 'Police Forces of El Salvador',
    'Police Forces of El Salvador (2009-2019)': 'Police Forces of El Salvador',
    'Military Forces of El Salvador (2019-)': 'Military Forces of El Salvador',
    'Military Forces of El Salvador (2009-2019)': 'Military Forces of El Salvador',
    'B-18 (S): Barrio-18 (Surenos)': 'B-18: Barrio-18',
    'B-18 (R): Barrio-18 (Revolucionarios)': 'B-18: Barrio-18',
    'Unidentified Gang (El Salvador)': 'Unidentified Group (El Salvador)',
    'Unidentified Armed Group (El Salvador)': 'Unidentified Group (El Salvador)'
}

#Apply the mapping to the `actor1` column
actors_df['cleaned_actor1'] = actors_df['actor1'].replace(actor_mapping)

#group by categories
tidy_actor_counts = (
    actors_df.groupby('cleaned_actor1')['count_actors']
    .sum()
    .reset_index()
    .sort_values(by='count_actors', ascending=False)
)

# Display the tidy result
print(f"Total unique actors after cleaning: {tidy_actor_counts['cleaned_actor1'].nunique()}")
print(tidy_actor_counts)
```

```{python}
el_salvador_data_2['cleaned_actor1'] = el_salvador_data_2['actor1'].replace(actor_mapping)

# Group by year and cleaned actor categories, then count occurrences
yearly_actor_counts = (
    el_salvador_data_2.groupby(['year', 'cleaned_actor1'])
    .size()
    .reset_index(name='count_actors')
)

```

```{python}
bar_chart_crime = alt.Chart(yearly_actor_counts).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('count_actors:Q', title='Actor Count'),
    color=alt.Color('cleaned_actor1', title='Actor'),
    tooltip=['cleaned_actor1', 'year', 'count_actors']
).transform_filter(
    alt.datum.count_actors > 20  # Filter for actors with count > 20
).properties(
    title="Actor Prevalence by Year (Count > 20)",
    width=600,
    height=400
)

# Display the chart
bar_chart_crime
```

```{python}
import gensim
from gensim import corpora
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
import re
import altair as alt
import pandas as pd


el_salvador_filtered = acled_2013[acled_2013['country'] == 'El Salvador']

## Preprocess
def preprocess_text_column(text):
    text = text.lower()
    text = re.sub(r'\W+', ' ', text)
    tokens = text.split()
    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS and len(word) > 3] 
    return tokens


el_salvador_filtered['notes_cleaned'] = el_salvador_filtered['notes'].apply(
    lambda x: preprocess_text_column(x) if isinstance(x, str) else []
)

#Build the dictionary/ corpus
lda_dictionary = corpora.Dictionary(el_salvador_filtered['notes_cleaned'])
lda_corpus = [lda_dictionary.doc2bow(text) for text in el_salvador_filtered['notes_cleaned']]

# Train LDA model on all years
lda_model = gensim.models.LdaModel(
    lda_corpus, 
    num_topics=5,  # Adjust as needed
    id2word=lda_dictionary, 
    passes=15
)

# Get the top words for each topic
def get_topic_top_words(lda_model, num_words=5):
    topics = lda_model.print_topics(num_topics=-1, num_words=num_words)
    topic_dict = {
        topic_id: [word.split("*")[1].strip('"') for word in topic_desc.split(" + ")]
        for topic_id, topic_desc in topics
    }
    return topic_dict

topic_words = get_topic_top_words(lda_model, num_words=5)

# Convert topics to a DataFrame for tabular display
topic_words_table = pd.DataFrame([
    {"Topic": topic_id, "Words": ", ".join(words)}
    for topic_id, words in topic_words.items()
])


def calculate_topic_distribution(document):
    topic_distribution = lda_model.get_document_topics(document, minimum_probability=0.0)
    return [prob for _, prob in topic_distribution]

el_salvador_filtered['topic_distribution'] = el_salvador_filtered['notes_cleaned'].apply(
    lambda x: calculate_topic_distribution(lda_dictionary.doc2bow(x))
)

# Reset index to ensure alignment
el_salvador_filtered = el_salvador_filtered.reset_index(drop=True)

# Create DataFrame from topic distributions
topic_columns = [f"Topic {i}" for i in range(lda_model.num_topics)]
topic_distributions_df = pd.DataFrame(
    el_salvador_filtered['topic_distribution'].tolist(), 
    columns=topic_columns
)

#concat back into df
el_salvador_filtered = pd.concat([el_salvador_filtered, topic_distributions_df], axis=1)

#aggregate topic proportions by year
topics_over_time = el_salvador_filtered.groupby('year')[topic_columns].mean().reset_index()

#visualize topic proportions over time
topics_melted = topics_over_time.melt(id_vars='year', var_name='Topic', value_name='Proportion')

topic_chart = alt.Chart(topics_melted).mark_line(point=True).encode(
    x='year:O',
    y='Proportion:Q',
    color='Topic:N',
    tooltip=['year', 'Topic', 'Proportion']
).properties(
    title="Topic Proportions Over Time",
    width=600,
    height=400
)

topic_chart
```

```{python}
print(topic_words_table)
```

```{python}
from collections import Counter

#  word frequencies for each year
def get_word_frequencies_by_year(df):
    word_frequencies = {}
    for year, group in df.groupby('year'):
        all_words = [word for text in group['notes_cleaned'] for word in text]
        word_counts = Counter(all_words)
        word_frequencies[year] = word_counts
    return word_frequencies

word_frequencies_by_year = get_word_frequencies_by_year(el_salvador_filtered)

# Convert to DF
word_freq_df = pd.DataFrame([
    {"Year": year, "Word": word, "Frequency": freq}
    for year, word_counts in word_frequencies_by_year.items()
    for word, freq in word_counts.items()
])

top_words_df = (
    word_freq_df.groupby("Year")
    .apply(lambda group: group.nlargest(10, "Frequency"))
    .reset_index(drop=True)
)

alt.Chart(top_words_df).mark_bar().encode(
    x=alt.X("Frequency:Q", title="Frequency"),
    y=alt.Y("Word:N", title="Word", sort="-x"),
    color="Year:N",
    column=alt.Column("Year:O", title="Year")
).properties(
    title="Top Words by Year",
    width=150,
    height=400
)

```

```{python}
# This is the column we want to track
print(acled_2013['location'].unique)
```

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt
### change directory for the shapefiles
os.chdir('d:\\UChicago\\Classes\\2024Qfall\\Programming Python\\Final-Project\\CA_shape_files') # change wd here
# GeoDataFrame for El Salvador administrative boundaries
el_sal_admin_boundaries = gpd.read_file(
    'slv_admbnda_adm3_gadm_20240819.shp'
)

# Ensure CRS is WGS84 for compatibility with event data
el_sal_admin_boundaries = el_sal_admin_boundaries.to_crs("EPSG:4326")

# Plot for verification
el_sal_admin_boundaries.plot(edgecolor="black", figsize=(10, 8))
plt.title("El Salvador Administrative Boundaries", fontsize=16)
plt.show()
```

```{python}
# GeoDataFrame for El Salvador event data
el_sal_event_data = gpd.GeoDataFrame(
    el_salvador_data_2,
    geometry=gpd.points_from_xy(el_salvador_data_2.longitude, el_salvador_data_2.latitude),
    crs="EPSG:4326"
)

```

```{python}
# Perform spatial join to assign events to administrative boundaries
el_sal_events_with_boundaries = gpd.sjoin(
    el_sal_event_data, el_sal_admin_boundaries, how="left", predicate="within"
)
```

```{python}
# Filter for relevant years
el_sal_events_filtered = el_sal_events_with_boundaries[
    el_sal_events_with_boundaries['year'].isin([2018, 2023])
]

# Aggregate event counts by administrative boundary and year
el_sal_events_aggregated = (
    el_sal_events_filtered.groupby(['ADM3_PCODE', 'year'])
    .size()
    .unstack(fill_value=0)
    .reset_index()
)

# Rename columns dynamically for better clarity
column_names = ['admin_boundary_code', 'event_count_2018', 'event_count_2023']
el_sal_events_aggregated.columns = column_names
```

```{python}
# Add percent change column
el_sal_events_aggregated['percent_change_events'] = (
    (el_sal_events_aggregated['event_count_2023'] - el_sal_events_aggregated['event_count_2018'])
    / el_sal_events_aggregated['event_count_2018'].replace(0, 1)  # Avoid division by zero
) * 100  # Convert to percentage
```

```{python}
# Merge event data with administrative boundaries
el_sal_admin_with_events = el_sal_admin_boundaries.merge(
    el_sal_events_aggregated, left_on='ADM3_PCODE', right_on='admin_boundary_code', how='left'
)

# Fill missing values
el_sal_admin_with_events['percent_change_events'] = el_sal_admin_with_events['percent_change_events'].fillna(0)
```

```{python}
nica_admin_boundaries = gpd.read_file(
    'geoBoundaries-NIC-ADM2.shp'
)


guat_admin_boundaries = gpd.read_file(
    'geoBoundaries-GTM-ADM2.shp'
)

hun_admin_boundaries = gpd.read_file(
    'geoBoundaries-HND-ADM2.shp'
)

panama_admin_boundaries = gpd.read_file(
    'whosonfirst-data-admin-pa-county-polygon.shp'
)


costa_rica_admin_boundaries = gpd.read_file(
    'geoBoundaries-CRI-ADM3.shp'
)

belize_admin_boundaries = gpd.read_file(
    'geoBoundaries-BLZ-ADM2.shp'
)


```

```{python}
print("Belize Administrative Boundaries:")
print(belize_admin_boundaries.head(), "\n")

print("Nicaragua Administrative Boundaries:")
print(nica_admin_boundaries.head(), "\n")

print("Guatemala Administrative Boundaries:")
print(guat_admin_boundaries.head(), "\n")

print("Honduras Administrative Boundaries:")
print(hun_admin_boundaries.head(), "\n")

print("Panama Administrative Boundaries:")
print(panama_admin_boundaries.head(), "\n")

print("Costa Rica Administrative Boundaries:")
print(costa_rica_admin_boundaries.head(), "\n")

print("El Salvador Administrative Boundaries:")
print(el_sal_admin_boundaries.head(), "\n")
```

```{python}
# Ensure CRS is consistent
desired_crs = "EPSG:4326"

# Prepare and normalize each GeoDataFrame
belize_admin_boundaries['country'] = 'Belize'
belize_admin_boundaries = belize_admin_boundaries.rename(columns={
    'shapeName': 'boundary_name', 
    'shapeID': 'boundary_id'
})[['country', 'boundary_name', 'boundary_id', 'geometry']].to_crs(desired_crs)

nica_admin_boundaries['country'] = 'Nicaragua'
nica_admin_boundaries = nica_admin_boundaries.rename(columns={
    'shapeName': 'boundary_name', 
    'shapeID': 'boundary_id'
})[['country', 'boundary_name', 'boundary_id', 'geometry']].to_crs(desired_crs)

guat_admin_boundaries['country'] = 'Guatemala'
guat_admin_boundaries = guat_admin_boundaries.rename(columns={
    'shapeName': 'boundary_name', 
    'shapeID': 'boundary_id'
})[['country', 'boundary_name', 'boundary_id', 'geometry']].to_crs(desired_crs)

hun_admin_boundaries['country'] = 'Honduras'
hun_admin_boundaries = hun_admin_boundaries.rename(columns={
    'shapeName': 'boundary_name', 
    'shapeID': 'boundary_id'
})[['country', 'boundary_name', 'boundary_id', 'geometry']].to_crs(desired_crs)

panama_admin_boundaries['country'] = 'Panama'
panama_admin_boundaries = panama_admin_boundaries.rename(columns={
    'name': 'boundary_name', 
    'id': 'boundary_id'
})[['country', 'boundary_name', 'boundary_id', 'geometry']].to_crs(desired_crs)

costa_rica_admin_boundaries['country'] = 'Costa Rica'
costa_rica_admin_boundaries = costa_rica_admin_boundaries.rename(columns={
    'shapeName': 'boundary_name', 
    'shapeID': 'boundary_id'
})[['country', 'boundary_name', 'boundary_id', 'geometry']].to_crs(desired_crs)

el_sal_admin_boundaries['country'] = 'El Salvador'
el_sal_admin_boundaries = el_sal_admin_boundaries.rename(columns={
    'ADM3_ES': 'boundary_name', 
    'ADM3_PCODE': 'boundary_id'
})[['country', 'boundary_name', 'boundary_id', 'geometry']].to_crs(desired_crs)

# Combine all GeoDataFrames
ca_admin_boundaries = gpd.GeoDataFrame(
    pd.concat([
        belize_admin_boundaries,
        nica_admin_boundaries,
        guat_admin_boundaries,
        hun_admin_boundaries,
        panama_admin_boundaries,
        costa_rica_admin_boundaries,
        el_sal_admin_boundaries
    ], ignore_index=True),
    crs=desired_crs
)

# Save to a shapefile or other formats if needed
##ca_admin_boundaries.to_file('ca_admin_boundaries1.shp', driver='ESRI Shapefile')

```

```{python}
# Save to Shapefile
#ca_admin_boundaries.to_file('ca_admin_boundaries.shp', driver='ESRI Shapefile')
```

```{python}
#ca_admin_boundaries.plot(edgecolor="black", figsize=(10, 8))
#plt.title("Central American Administrative Boundaries", fontsize=16)
plt.show()
```

```{python}
#ca_gdf = gpd.read_file(
    #'/Users/willsigal/Documents/GitHub/Final-Project/CA_shape_files/ca_admin_boundaries.shp')

#ca_gdf.head()
```

Time_Series_Analysis
```{python}
import statsmodels.api as sm
# Download data
os.chdir('d:\\UChicago\\Classes\\2024Qfall\\Programming Python\\Final-Project\\Data') # change wd here
df_acled = pd.read_csv("acled_project_data_cleaned.csv")
```

```{python}
# Ensure 'event_date' is in datetime format
df_acled['event_date'] = pd.to_datetime(df_acled['event_date'])

# Create a quarter column based on the event_date
df_acled['quarter'] = df_acled['event_date'].dt.to_period('M').dt.start_time
# I originally did it for quarters but switched it up to months in the line above. 
# Did not change the column names to month but can do this later if we stick to months.

# Count crimes per country and quarter
attack_counts = df_acled.groupby(['country', 'quarter']).size().reset_index(name='attack_count')

# Exclude mexico (outlier)
attack_counts = attack_counts[attack_counts['country'] != 'Mexico']

# Save the quarters as strings
attack_counts['quarter'] = attack_counts['quarter'].astype(str)

# Create a DataFrame for policy change annotations
policy_changes = pd.DataFrame({
    'policy_name': [
        'Territorial Control Plan', 
        'COVID Pandemic', 
        'State of Emergency',
        'Mass Incarceration'
    ],
    'policy_date': [
        '2019-06-01', 
        '2020-03-18',
        '2022-03-27',
        '2023-02-01'
    ]
})

# Convert policy_date to datetime
policy_changes['policy_date'] = pd.to_datetime(policy_changes['policy_date'])

# Create the chart
quarter_attacks_line_chart = alt.Chart(attack_counts).mark_line(point=False).encode(
    x=alt.X('quarter:T', title='', axis=alt.Axis(format='%b %Y', labelAngle=30, labelOverlap=False)),
    y=alt.Y('attack_count:Q', title='', scale=alt.Scale(domain=[0, 140])),
    color=alt.Color('country:N', title='Country')
).properties(
    title='Monthly Attacks By Central American Country',
    width=650,
    height=400
)

# Vertical lines for interventions
policy_lines = alt.Chart(policy_changes).mark_rule(color='red', strokeDash=[1, 2], strokeWidth = 5).encode(
    x='policy_date:T',
    tooltip=['policy_name', 'policy_date']
)

# Text labels for interventions
policy_labels = alt.Chart(policy_changes).mark_text(align='left', dx= 5, dy=-190, fontSize=8).encode(
    x='policy_date:T',
    text='policy_name:N'
)

# Combine the base chart, policy lines, and labels
final_chart = quarter_attacks_line_chart + policy_lines + policy_labels

# Display the chart
final_chart.display()
```

```{python}
# Only include El Salvador
attack_counts = attack_counts[attack_counts['country'].isin(['El Salvador'])]

# Prepare for OLS regression
attack_counts['quarter'] = pd.to_datetime(attack_counts['quarter'])
attack_counts = attack_counts.sort_values(['country', 'quarter'])
attack_counts['time_numeric'] = (attack_counts['quarter'] - attack_counts['quarter'].min()).dt.days

# Run OLS 
X = sm.add_constant(attack_counts['time_numeric'])
y = attack_counts['attack_count']
model = sm.OLS(y, X).fit()
attack_counts['ols_prediction'] = model.predict(X)

# Define policy interventions
policy_changes = pd.DataFrame({
    'policy_name': [
        'Territorial Control Plan', 
        'COVID Pandemic', 
        'State of Emergency',
        'Mass Incarceration'
    ],
    'policy_date': [
        '2019-06-01', 
        '2020-03-18',
        '2022-03-27',
        '2023-02-01'
    ]
})
policy_changes['policy_date'] = pd.to_datetime(policy_changes['policy_date'])

# Create the  chart
quarter_attacks_line_chart = alt.Chart(attack_counts).mark_line(point=True).encode(
    x=alt.X('quarter:T', title='', axis=alt.Axis(format='%b %Y', labelAngle=30, labelOverlap=False)),
    y=alt.Y('attack_count:Q', title='', scale=alt.Scale(domain=[0, 100])),
    color=alt.Color('country:N', title='Country', legend = None)
).properties(
    title='Monthly El Salvador Attacks With Period-Invariant OLS Regression Line',
    width=700,
    height=400
)

# Add the OLS regression line
ols_line = alt.Chart(attack_counts).mark_line(color='black', strokeDash=[5, 3], strokeWidth=2).encode(
    x='quarter:T',
    y='ols_prediction:Q',
    tooltip=['quarter:T', 'ols_prediction:Q']
)

# Add vertical lines for policy interventions
policy_lines = alt.Chart(policy_changes).mark_rule(color='red', strokeDash=[1, 2], strokeWidth = 5).encode(
    x='policy_date:T',
    tooltip=['policy_name', 'policy_date']
)

# Add text labels for policy interventions
policy_labels = alt.Chart(policy_changes).mark_text(align='left', dx=5, dy=-190, fontSize=8).encode(
    x='policy_date:T',
    text='policy_name:N'
)

# Combine all layers
final_chart = quarter_attacks_line_chart + ols_line + policy_lines + policy_labels

# Display the chart
final_chart.display()
```

```{python}
# Divide data into time periods based on policy dates
policy_changes = policy_changes.sort_values('policy_date')
policy_dates = policy_changes['policy_date'].tolist()

# Add start and end dates to cover full range
start_date = attack_counts['quarter'].min()
end_date = attack_counts['quarter'].max()
policy_dates = [start_date] + policy_dates + [end_date]

# Segment data into periods
attack_counts['period'] = pd.cut(
    attack_counts['quarter'], 
    bins=policy_dates, 
    labels=[f"Period {i}" for i in range(len(policy_dates)-1)],
    include_lowest=True
)

# Perform OLS regression for each period and country (keep flexible for many countries)
ols_period_predictions = []
for (country, period), period_data in attack_counts.groupby(['country', 'period']):
    period_data['time_numeric'] = (period_data['quarter'] - period_data['quarter'].min()).dt.days
    X = sm.add_constant(period_data['time_numeric'])
    y = period_data['attack_count']
    
    if len(period_data) > 1:  
        model = sm.OLS(y, X).fit()
        period_data['ols_prediction'] = model.predict(X)
    else:
        period_data['ols_prediction'] = None

    period_data['regression_period'] = f"{country} {period}"
    ols_period_predictions.append(period_data)

# Combine all predictions back into a single DataFrame
attack_counts_with_period_ols = pd.concat(ols_period_predictions)

# Create the base chart for actual data
quarter_attacks_line_chart = alt.Chart(attack_counts_with_period_ols).mark_line(point=True).encode(
    x=alt.X('quarter:T', title='', axis=alt.Axis(format='%b %Y', labelAngle=30, labelOverlap=False)),
    y=alt.Y('attack_count:Q', title='', scale=alt.Scale(domain=[0, 100])),
    color=alt.Color('country:N', title='Country', legend = None),
).properties(
    title='Monthly El Salvador Attacks with Period-Variant OLS Regression Lines',
    width=700,
    height=400
)

# Add the OLS regression lines for each period
ols_lines = alt.Chart(attack_counts_with_period_ols).mark_line(
    color='black', 
    strokeDash=[5, 3], 
    strokeWidth=2
).encode(
    x=alt.X('quarter:T'),
    y=alt.Y('ols_prediction:Q'),
    detail='regression_period:N', 
)

# Vertical lines for policy interventions
policy_lines = alt.Chart(policy_changes).mark_rule(color='red', strokeDash=[1, 2], strokeWidth = 5).encode(
    x='policy_date:T',
    tooltip=['policy_name', 'policy_date']
)

# Text labels for policy interventions
policy_labels = alt.Chart(policy_changes).mark_text(align='left', dx=5, dy=-190, fontSize=8).encode(
    x='policy_date:T',
    text='policy_name:N'
)

# Combine all layers
final_chart_with_period_ols = quarter_attacks_line_chart + ols_lines + policy_lines + policy_labels

# Display the chart
final_chart_with_period_ols.display()
```